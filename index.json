[
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AWS Well-Architected Security Pillar” Event Event Objectives Understand the Security Pillar of the AWS Well-Architected Framework: Gain insights into security best practices and how they apply to cloud architectures, focusing on AWS services and tools.\nLearn core security principles: Explore key principles such as Least Privilege, Zero Trust, and Defense in Depth to implement robust security in cloud environments.\nRecognize cloud security threats in Vietnam: Discuss common security challenges and threats faced by organizations in Vietnam’s cloud environment.\nDive deep into key AWS security pillars: Understand the five key security pillars—Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response—and how to implement them effectively.\nKey Highlights Opening \u0026amp; Security Foundation Security Pillar in Well-Architected Framework: Introduction to the importance of the Security Pillar in the AWS Well-Architected Framework and how it ensures a secure architecture.\nCore principles: Focus on Least Privilege, Zero Trust, and Defense in Depth as foundational concepts for designing secure systems.\nShared Responsibility Model: Understanding the division of responsibility between AWS and the customer for security in the cloud.\nTop threats in Vietnam\u0026rsquo;s cloud environment: Identifying the most common security threats faced by organizations operating in Vietnam\u0026rsquo;s cloud environment.\nPillar 1 — Identity \u0026amp; Access Management Modern IAM Architecture: Overview of Identity and Access Management (IAM) in AWS, including users, roles, and policies, and the importance of avoiding long-term credentials.\nIAM Identity Center: Introduction to Single Sign-On (SSO) and permission sets for managing access across AWS accounts.\nSCP \u0026amp; Permission Boundaries: How to use Service Control Policies (SCP) and permission boundaries to manage multi-account environments securely.\nMFA, Credential Rotation, Access Analyzer: Techniques for ensuring secure access through Multi-Factor Authentication (MFA), credential rotation, and access analysis.\nMini Demo: Validate IAM Policy and simulate access to see how IAM works in practice.\nPillar 2 — Detection Detection \u0026amp; Continuous Monitoring: Introduction to key services like CloudTrail, GuardDuty, and Security Hub for monitoring and detecting security events in AWS environments.\nLogging at every layer: Discuss the importance of logging at different layers (e.g., VPC Flow Logs, ALB/S3 logs) for comprehensive security monitoring.\nAlerting \u0026amp; Automation with EventBridge: Learn how to set up EventBridge for automated alerting and incident responses.\nDetection-as-Code: Implementing infrastructure and security rules using Detection-as-Code to automate the detection process.\nPillar 3 — Infrastructure Protection Network \u0026amp; Workload Security: Understanding the importance of VPC segmentation and private vs public placement in securing network traffic.\nSecurity Groups vs NACLs: Discuss the difference between Security Groups and Network Access Control Lists (NACLs) and which model is appropriate for different scenarios.\nWAF + Shield + Network Firewall: Protecting applications and networks using AWS WAF (Web Application Firewall), Shield, and Network Firewall to mitigate threats.\nWorkload Protection: Best practices for securing EC2 instances, ECS, and EKS clusters to ensure workload security.\nPillar 4 — Data Protection Encryption, Keys \u0026amp; Secrets: Introduction to AWS KMS (Key Management Service), including key policies, grants, and key rotation for secure data management.\nEncryption at Rest \u0026amp; in Transit: How to ensure encryption of data at rest (e.g., S3, EBS, RDS) and in transit (e.g., DynamoDB).\nSecrets Management: Using Secrets Manager and Parameter Store for managing sensitive data like passwords, keys, and tokens with automated rotation patterns.\nData Classification \u0026amp; Access Guardrails: Implementing data classification and access controls to ensure data protection.\nPillar 5 — Incident Response IR Playbook \u0026amp; Automation: The lifecycle of an Incident Response (IR) according to AWS, and how to automate the response process using AWS tools.\nIR Playbook: Common scenarios like a compromised IAM key, S3 public exposure, and EC2 malware detection, and how to handle them effectively.\nAuto-response with Lambda/Step Functions: Using AWS Lambda and Step Functions to automate incident response actions and mitigate damage quickly.\nQ\u0026amp;A \u0026amp; Wrap-up Summary of the 5 Pillars: A recap of the five key security pillars and their application in real-world AWS environments.\nCommon Pitfalls \u0026amp; Practices in Vietnamese Enterprises: Discussing common security challenges faced by Vietnamese companies and how to address them with AWS tools.\nSecurity Learning Roadmap: Overview of Security Specialty certification and Solutions Architect – Professional (SA Pro) certification, providing a learning path for security experts.\nWhat I Learned Security Principles \u0026amp; Pillars Core security principles: Understanding the importance of Least Privilege, Zero Trust, and Defense in Depth in the context of cloud security.\nAWS Well-Architected Security Pillars: The significance of each security pillar—Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response—in ensuring a secure cloud environment.\nCI/CD Pipeline with AWS: AWS CodeCommit: Best practices for managing source code using AWS’s version control system.\nCodeBuild \u0026amp; CodeDeploy: How to configure automated build and deployment pipelines with AWS tools.\nCodePipeline Automation: The role of AWS CodePipeline in automating the entire software delivery lifecycle.\nIAM \u0026amp; Access Control Modern IAM Architecture: Using IAM effectively to manage access securely, with tools like IAM Identity Center for SSO and MFA for strong authentication.\nMulti-account Security: How to implement SCP and permission boundaries for securing multi-account AWS environments.\nContinuous Monitoring \u0026amp; Detection CloudTrail, GuardDuty, and Security Hub: Setting up CloudTrail for activity tracking, using GuardDuty for threat detection, and aggregating security findings in Security Hub.\nLogging \u0026amp; Automation: Best practices for comprehensive logging and automated alerting with EventBridge.\nMonitoring and Observability: CloudWatch \u0026amp; X-Ray: How to implement full-stack observability in a microservices architecture using AWS CloudWatch and X-Ray for better performance tracking and issue resolution. Data Protection KMS \u0026amp; Encryption: How to implement key management and data encryption to secure sensitive data in the cloud.\nSecrets Management: Best practices for securely managing and rotating secrets with Secrets Manager and Parameter Store.\nIncident Response Automating Incident Response: Using AWS Lambda and Step Functions to automate incident response processes, improving speed and efficiency during security breaches. Application to My Work IAM Management: Implement stronger IAM policies and role-based access control (RBAC) in my organization to ensure secure and granular access management.\nContinuous Monitoring: Set up CloudTrail, GuardDuty, and Security Hub for ongoing monitoring and alerting of potential security threats.\nIncident Response Automation: Automate incident response processes using Lambda and Step Functions to reduce reaction time and improve security efficiency.\nData Encryption: Apply encryption for both data-at-rest and data-in-transit, ensuring that all sensitive data is securely handled.\nMulti-Account Security: Use SCPs and permission boundaries to manage security across multiple AWS accounts.\nExperience at the Event Learning from AWS Experts: The session provided deep insights into cloud security best practices and AWS security tools, which will help me implement effective security measures in my cloud architecture.\nHands-on Demos: I particularly enjoyed the mini demo for validating IAM policies and the incident response demo using Lambda and Step Functions, which provided practical knowledge for my own work.\nNetworking \u0026amp; Knowledge Sharing: The event allowed me to interact with security professionals, expanding my understanding of the challenges specific to cloud security in Vietnam.\nConclusion The “AWS Well-Architected Security Pillar” event was incredibly informative, providing a comprehensive understanding of cloud security best practices. The key takeaways about IAM management, continuous monitoring, data protection, and incident response automation will be directly applicable to securing our AWS environments and improving our incident response times. I now have a clearer roadmap for enhancing security practices in my organization using AWS tools.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Summary Report: “DevOps on AWS” Event Event Objectives Provide an overview of AI/ML concepts and their applications within the DevOps environment.\nIntroduce and clarify the DevOps culture and core principles that enhance team collaboration and automation.\nUnderstand key DevOps performance metrics such as DORA metrics, MTTR, and deployment frequency to evaluate and improve processes.\nSupport interns in acquiring essential knowledge and effectively applying it to real projects to complete tasks on schedule and with quality.\nKey Highlights 1. Recap AI/ML Session Summarize basic AI/ML concepts and practical applications in software development and DevOps.\nExplore AI/ML tools and services that support automation and data analytics in DevOps workflows.\n2. DevOps Culture and Principles Introduce DevOps culture: cross-team collaboration, automation, and continuous improvement.\nCore DevOps principles that accelerate development speed and enhance software reliability.\n3. Benefits and Key Metrics Importance of measuring DevOps performance through key metrics:\nDORA metrics: Lead Time, Deployment Frequency, Change Failure Rate, Mean Time to Recovery (MTTR).\nThe meaning of these metrics and how to improve them for optimized development and operations processes.\n4. Practical Applications for Interns Guide on applying DevOps and AI/ML knowledge to projects.\nTips and best practices to help interns complete projects efficiently.\nEnhance teamwork and communication skills in the DevOps environment.\nPillar 1 — AWS DevOps Services - CI/CD Pipeline Present modern IAM architecture including users, roles, policies, emphasizing avoiding long-term credentials to reduce risk.\nIntroduce IAM Identity Center with Single Sign-On (SSO) and permission sets for unified access management across multiple AWS accounts.\nExplain the roles of Service Control Policies (SCP) and permission boundaries in controlling permissions in multi-account environments to enhance security.\nPresent use of Multi-Factor Authentication (MFA), credential rotation, and Access Analyzer to secure and accurately manage access.\nDemo illustrating IAM policy validation and access simulation for practical understanding of IAM in AWS.\nPillar 2 — Infrastructure as Code (IaC) Introduce monitoring and security tools like CloudTrail (API history logging), GuardDuty (threat detection), and Security Hub (aggregated security alerts).\nPractice detailed logging across multiple layers such as VPC Flow Logs, ALB/S3 logs to ensure comprehensive monitoring and early incident detection.\nGuide on using EventBridge to set up automated alerts and trigger response actions upon security events.\nApply Detection-as-Code: build automated detection rules through code to increase automation and efficiency in security management.\nPillar 3 — Container Services on AWS Learn to protect networks and workloads on AWS via VPC segmentation and separation of private/public traffic to minimize public network risks.\nCompare and apply Security Groups and Network ACLs (NACLs) appropriately for traffic control.\nIntroduce AWS services protecting application and network layers like AWS WAF (Web Application Firewall), Shield (DDoS protection), and Network Firewall.\nPresent best security practices for EC2, ECS, and EKS workloads to maintain safe containerized and virtual server environments.\nPillar 4 — Monitoring \u0026amp; Observability Introduce AWS Key Management Service (KMS) for encryption key management, including policies, permissions, and key rotation to protect data.\nPresent encryption techniques for data at rest (S3, EBS, RDS) and in transit (e.g., DynamoDB, other AWS services).\nGuide sensitive data management via Secrets Manager and Parameter Store with automated rotation to reduce risk.\nDiscuss data classification and access control measures to ensure data is accessed only by authorized users and systems.\nPillar 5 — DevOps Best Practices \u0026amp; Case Studies Introduce Incident Response (IR) implementation following AWS best practices combined with automation for fast security incident handling.\nPresent IR playbooks for common scenarios such as leaked IAM keys, public S3 buckets, or malware-infected EC2 instances, including detailed remediation steps.\nDemonstrate AWS Lambda and Step Functions usage for automated incident response to reduce detection and recovery times.\nSummary \u0026amp; Q\u0026amp;A Recap the 5 main AWS security pillars and their practical applications to help enterprises build secure, efficient cloud systems.\nDiscuss common security mistakes and challenges in Vietnamese enterprises and propose AWS-based solutions.\nIntroduce learning paths and certifications in security such as AWS Security Specialty and Solutions Architect – Professional (SA Pro) for skill advancement.\nWhat I Learned DevOps Culture and Principles Deepened understanding of DevOps culture and principles that foster cross-team collaboration, automation, and continuous improvement. Grasped the importance of measuring DevOps effectiveness using key metrics like DORA metrics and MTTR to improve project quality. AI/ML in DevOps Identified AI/ML applications that enable automation and data analysis in DevOps pipelines. Learned how AI/ML supports pipeline optimization and rapid incident detection. AWS DevOps Services and Security Studied modern IAM architecture and secure access management with IAM Identity Center, SCPs, and permission boundaries. Practiced MFA, credential rotation, and Access Analyzer to enhance account and service security. Participated in demos for policy validation and access simulation within AWS IAM. Continuous Monitoring and Detection Applied AWS tools like CloudTrail, GuardDuty, and Security Hub for comprehensive monitoring and security incident detection. Configured multi-layer logging and automated alerting with EventBridge for rapid incident response. Network and Container Management Learned VPC segmentation, and effective use of Security Groups and NACLs to safeguard EC2, ECS, and EKS workloads. Used AWS WAF, Shield, and Network Firewall for layered network and application protection. Data Protection and Encryption Utilized AWS KMS for encryption key management, securing data at rest and in transit. Managed secrets and automated rotation with Secrets Manager and Parameter Store. Applied data classification and strict access controls. Automated Incident Response Built Incident Response playbooks covering detection, analysis, containment, and recovery steps. Automated incident handling using AWS Lambda and Step Functions to minimize damage. Learned to manage typical cases like IAM key leaks, public S3 buckets, and infected EC2 instances. Practical Applications Applied IAM and access management knowledge in real project contexts. Established monitoring and automated alert systems to enhance AWS environment security. Integrated automated incident response to save time and reduce risks. Implemented data encryption and secret management following best security practices. Event Experience Learned directly from AWS experts and engaged in practical demos on security and DevOps. Connected and shared knowledge with the DevOps and security community, expanding professional expertise. Received practical advice and guidance tailored for personal work and projects. Conclusion The “DevOps on AWS” event gave me a comprehensive understanding of DevOps culture, AI/ML applications, and AWS security best practices. Knowledge of IAM, monitoring, data protection, and incident response forms a critical foundation that I will apply in projects, especially supporting interns to complete work efficiently and securely.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AI/ML/GenAI on AWS” Event Event Objectives Explore AWS AI/ML Services: Provide an overview of the AI/ML services available on AWS, helping Attendees understand how to apply these services in real-world projects.\nIntroduction to Generative AI: Focus on leveraging foundational models and Generative AI within AWS to build intelligent applications.\nLive Demo of Generative AI: Guide on building a chatbot using Amazon Bedrock, including techniques like prompt engineering and chain-of-thought reasoning.\nProvide Knowledge on MLOps and SageMaker: Introduce tools that manage the entire machine learning model lifecycle, from data preparation, model training to deployment and monitoring.\nKey Highlights Introduction to AI/ML on AWS: Amazon SageMaker: A comprehensive machine learning platform that helps deploy, train, and manage machine learning models.\nData Preparation and Labeling: Tools to assist with data preparation and labeling for machine learning models.\nMLOps Capabilities: Integrated features in SageMaker to support model operations (MLOps).\nLive Demo – SageMaker Studio Walkthrough: A demonstration of using SageMaker Studio to develop and deploy AI/ML models. Generative AI with Amazon Bedrock: Foundation Models: An introduction to foundational models such as Claude, Llama, Titan, and a guide to choosing the appropriate model.\nPrompt Engineering: Techniques for building effective prompts to optimize the performance of Generative AI models, including Chain-of-Thought reasoning and Few-shot learning.\nRetrieval-Augmented Generation (RAG): Architecture and integration of a knowledge base into the model generation process.\nBedrock Agents: Building multi-step workflows and integrating tools within Amazon Bedrock.\nGuardrails: Safety and content filtering measures when using Generative AI.\nLive Demo – Generative AI Chatbot with Bedrock: A demonstration of building a chatbot using Amazon Bedrock. What I Learned Design Thinking with AI/ML: Amazon SageMaker is the key platform supporting the full model lifecycle, from data preparation to model deployment and monitoring.\nPrompt Engineering: How to optimize Generative AI models using effective prompt-building techniques.\nMLOps: How to manage and automate model training, deployment, and monitoring processes in production environments.\nGenerative AI: Understanding foundational models and how they can create intelligent content, such as chatbots.\nAI/ML Architecture: Foundation Models: The differences between models like Claude, Llama, and Titan, and how to choose the right model based on project requirements.\nRAG (Retrieval-Augmented Generation): Understanding how RAG architecture works and how to integrate knowledge bases into the content creation process.\nBedrock Agents: Guidance on building complex workflows with pre-integrated tools.\nApplication to My Work Apply Amazon SageMaker: Integrate SageMaker tools into the data preparation, model training, and deployment processes for current projects.\nBuild a Generative AI Chatbot: Use Amazon Bedrock to develop chatbots and Generative AI applications for customer service or virtual assistant projects.\nOptimize AI/ML Models with Prompt Engineering: Apply prompt engineering techniques to improve AI models for better results.\nMLOps: Implement MLOps within the company to automate model training and monitoring, improving efficiency and reducing risks.\nExperience at the Event Learning from AWS Experts: The speakers shared in-depth knowledge about AI/ML and Generative AI, which helped me better understand how to use AWS tools for developing models.\nLive Demonstrations: I participated in live demos of SageMaker and Amazon Bedrock, which helped me visualize how to build and deploy AI models in real-world environments.\nNetworking and Interaction: The event allowed me to connect with other Attendees and experts, expanding my professional network and exchanging valuable AI/ML insights.\nConclusion The “AI/ML/GenAI on AWS” event provided me with valuable insights into the latest technologies in AI/ML, particularly around Generative AI and tools like SageMaker and Amazon Bedrock. Techniques like Prompt Engineering and MLOps have shown me the necessary steps to bring AI models from concept to production. I now feel more confident in applying these tools and techniques to real-world projects and advancing our AI/ML capabilities.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AI-Driven Development Life Cycle: Reimagining Software Engineering” Event Event Objectives Explore the AI-Driven Development Life Cycle and how AI is transforming modern software development processes.\nLearn how Amazon Q Developer and Kiro support developers in requirement analysis, code generation, testing, and performance optimization.\nIdentify real-world applications of AI in software development in Vietnam: increased productivity, reduced errors, and improved deployment speed.\nExplore the stages of the AI-driven software development life cycle: Requirement → Design → Coding → Testing → Deployment → Improvement.\nKey Content Opening \u0026amp; Software Engineering Foundation Overview of the transition from Traditional Software Development to AI-Driven Development.\nCore principles of software development when applying AI: transparency, verifiability, and effective collaboration between humans and AI models.\nAI-Augmented Engineer: a model where developers leverage AI to increase productivity and reduce development time.\nChallenges faced by Vietnamese companies in adopting AI for Software Engineering: workflow adaptation, output quality, and integration with existing systems.\nPillar 1 — AI-Driven Development Life Cycle overview and Amazon Q Developer demonstration Detailed introduction of the AI-Driven SDLC, including requirement analysis, design, code generation, automated testing, deployment, and continuous improvement.\nAmazon Q Developer:\nAutomatically generates code from technical specifications. Suggests bug fixes, analyzes logs, and provides improvement recommendations. Generates test cases and documentation automatically. Supports architecture generation and AWS service–related code. Mini Demo: Using Amazon Q Developer to build APIs, write backend code, generate unit tests, and optimize code in real time.\nPillar 2 — Kiro Demonstration Introduction to Kiro – an AI assistant designed for enterprise Software Engineering workflows.\nContinuous analysis and assistance:\nAnalyzes backlog items, requirement documents, and Agile processes. Suggests architectures, technical solutions, and user stories. Optimizes development workflows across Dev – QA – Ops. Workflow automation:\nGenerates Technical Specifications. Recommends codebase improvements and CI/CD process enhancements. Helps track progress and identify bottlenecks. Mini Demo: Using Kiro to analyze requirements and generate design documents and development plans.\nSummary \u0026amp; Q\u0026amp;A Summary of two pillars: how AI supports the entire software development life cycle from analysis to deployment.\nCommon challenges for Vietnamese companies: over-reliance on AI, inconsistent output quality, and the need for developer evaluation skills.\nLearning path: Amazon Q Developer, AWS AI Practitioner, Developer Associate, and specialized courses in GenAI for Software Engineering.\nKey Takeaways Overview of AI-Driven Development Gained a clear understanding of how AI is reshaping the software development process, from requirement analysis to deployment and maintenance. Recognized the “AI-Augmented Engineer” model—where AI assists developers in accelerating development, ensuring code quality, and reducing errors. Amazon Q Developer Learned to use Amazon Q Developer to generate source code from descriptions, helping speed up coding and reduce repetitive work. Understood how Q Developer interprets codebases, proposes refactoring, generates test cases, and analyzes errors. Practiced mini-demos involving generating APIs, handling backend logic, and automatically creating tests using AI. Kiro and Engineering Process Understood how Kiro assists engineering teams in managing the development life cycle: planning, requirement analysis, documentation generation, and architecture recommendations. Observed demos where Kiro analyzes backlogs, generates user stories, and proposes suitable technical solutions. Recognized how AI improves collaboration between Dev – QA – Ops. AI-Driven Software Development Life Cycle Understood all major stages: Requirement → Design → Coding → Testing → Deployment → Improvement and how AI contributes to each. Learned how AI supports automated testing and continuous quality monitoring. Application to Work Applied AI to write code faster, reduce bugs, and increase efficiency during feature development. Used tools like Q Developer to analyze logs, debug, and optimize code performance. Increased productivity in writing technical documentation (design docs, user stories) with AI. Automated repetitive tasks such as refactoring, test writing, and code reviewing. Event Experience Hands-on experience with demos from Amazon Q Developer and Kiro, gaining real-world insights into AI applications. Learned from AWS experts about modern trends in Software Engineering combined with GenAI. Connected with a community of developers applying AI in Vietnam. Conclusion The event “AI-Driven Development Life Cycle: Reimagining Software Engineering” provided deeper insight into applying AI in software development. With Amazon Q Developer and Kiro, developers can accelerate product development, improve code quality, and optimize the entire software development life cycle.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Learn about Compute VM services on AWS. Practice with Compute VM services on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Compute VM services on AWS. + Amazon Elastic Compute Cloud (EC2) + Amazon Lightsail + Amazon EFS / FSX + AWS Application Migration Service (MGN)regulations 09/15/2025 09/16/2025 https://www.youtube.com/watch?v=-t5h4N6vfBs\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=72 3 - Learn about Compute VM services on AWS. + Amazon Elastic Compute Cloud (EC2) + Amazon Lightsail + Amazon EFS / FSX + AWS Application Migration Service (MGN)regulations 09/15/2025 09/16/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000013.awsstudygroup.com/vi 5 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000024.awsstudygroup.com 6 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000024.awsstudygroup.com Week 3 Achievements: Understood AWS Compute VM services:\nAmazon Elastic Compute Cloud (EC2): Amazon EC2 is similar to a virtual or traditional physical server. It provides fast provisioning, strong resource scalability, and flexibility. Amazon Lightsail: a low-cost compute service (pricing starts at $3.5/month). Each Lightsail instance includes a data transfer allocation (cheaper than EC2), suitable for light workloads, dev/test environments, and applications that do not require high CPU usage continuously for more than 2 hours per day. Amazon EFS / FSx: EFS (Elastic File System): allows creation of NFSv4 network volumes that can be mounted to multiple EC2 instances simultaneously, with storage scaling up to petabytes. EFS only supports Linux and charges based on used storage. It can be mounted to on-premises environments via Direct Connect or VPN. FSx: allows creation of NTFS volumes mountable to multiple EC2 instances using the SMB protocol, supports Windows and Linux, and charges based on used storage. AWS Application Migration Service: used to migrate and replicate servers for building Disaster Recovery sites, continuously copying source physical or virtual servers to EC2 instances in AWS (asynchronously or synchronously). Successfully deployed AWS Backup, File Storage Gateway, and Auto Scaling Group..\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Discuss and brainstorm project ideas with the group. Learn about storage services on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about storage services on AWS. + Amazon Simple Storage Service - S3 + Amazon Storage Gateway + Snow Family + Disaster Recovery on AWS emsp; + AWS Backup 09/22/2025 09/23/2025 https://www.youtube.com/watch?v=hsCfP0IxoaM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=103 3 - Learn about storage services on AWS. + Amazon Simple Storage Service - S3 + Amazon Storage Gateway + Snow Family + Disaster Recovery on AWS emsp; + AWS Backup 09/22/2025 09/23/2025 https://www.youtube.com/watch?v=hsCfP0IxoaM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=103 4 - Perform Lab on AWS storage services. - Practice: + VM Import/Export + Deploy File Storage Gateway 09/24/2025 09/25/2025 https://000014.awsstudygroup.com/vi https://000024.awsstudygroup.com/vi 5 - Perform Lab on AWS storage services. - Practice: + VM Import/Export + Deploy File Storage Gateway 09/24/2025 09/25/2025 https://000014.awsstudygroup.com/vi https://000024.awsstudygroup.com/vi 6 - Group meeting about project ideas and writing worklog 09/26/2025 09/26/2025 Week 4 Achievements: Learned about AWS storage services:\nAmazon Simple Storage Service - S3 Amazon Storage Gateway Snow Familys Disaster Recovery on AWS Successfully imported/exported VM and deployed File Storage Gateway.\nCompleted writing the worklog and finalized the project idea.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Learn about security services on AWS. Do the lab. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about security services on AWS. + Shared Responsibility Model - AWS Identity and Access Management + Amazon Cognito + AWS Organization \u0026amp; AWS Identity Center ( SSO ) + AWS KMS 09/29/2025 09/30/2025 https://www.youtube.com/watch?v=tsobAlSg19g\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=150 3 - Learn about security services on AWS. + Shared Responsibility Model - AWS Identity and Access Management + Amazon Cognito + AWS Organization \u0026amp; AWS Identity Center ( SSO ) + AWS KMS 09/29/2025 09/30/2025 https://www.youtube.com/watch?v=tsobAlSg19g\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=150 4 Practice: + Compete Lab 18 about AWS Security Hub 10/01/2025 10/01/2025 https://000018.awsstudygroup.com 5 Practice: + Compete lab 22 \u0026amp; 27 about Optimizing EC2 Costs with Lambda và Manage Resources Using Tags and Resource Groups 10/02/2025 10/03/2025 https://000027.awsstudygroup.com https://000022.awsstudygroup.com 6 Practice: + Compete lab 22 \u0026amp; 27 about Optimizing EC2 Costs with Lambda và Manage Resources Using Tags and Resource Groups 10/02/2025 10/03/2025 https://000027.awsstudygroup.com https://000022.awsstudygroup.com Week 5 Achievements: Understand AWS security services:\nShared Responsibility Model: The AWS security model defines the division of responsibilities between AWS and customers in protecting systems and data on the cloud platform. AWS Identity and Access Management (IAM): Manages user identities, roles, and permissions to securely control access to AWS resources. Amazon Cognito: Provides authentication, authorization, and user management for web and mobile applications. AWS Organization \u0026amp; AWS Identity Center (SSO): Enables centralized management of multiple AWS accounts, unified access control, and single sign-on for users across the organization. AWS KMS: Manages encryption keys used to protect data, allowing secure creation, storage, and control of keys. Understand the structure of AWS Security Hub.\nSuccessfully completed Lab 22 \u0026amp; 27.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Continue working on the labs from Module 5. Understand basic AWS services and how to use both the AWS Management Console and AWS CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Practice: + Do lab 28 about MANAGE ACCESS TO EC2 SERVICES WITH RESOURCE TAGS THROUGH IAM SERVICES 10/06/2025 10/06/2025 https://000028.awsstudygroup.com 3 Practice: + Do lab 30 \u0026amp; 33 about LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY and Encrypt at rest with AWS KMS 10/07/2025 10/07/2025 https://000030.awsstudygroup.com https://000033.awsstudygroup.com 4 Practice: + Do lab 44 \u0026amp; 48 about IAM Role \u0026amp; Condition and Granting authorization for an application to access AWS services with an IAM role. 10/08/2025 10/08/2025 https://000044.awsstudygroup.com https://000048.awsstudygroup.com 5 - Learn about Database Services on AWS: + Database Concepts + Amazon RDS + Amazon Aurora + Amazon RedShift + Amazon ElastiCache 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=OOD2RwWuLRw\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=217 6 - Group meeting to draw the system diagram 10/10/2025 10/10/2025 Week 6 Achievements: Learned how to manage access to EC2 services using Resource Tags with AWS IAM. Successfully completed Lab 30 \u0026amp; 33, understanding IAM Permission Boundaries (limiting user permissions) and data encryption at rest using AWS KMS. Gained deeper understanding of IAM Roles \u0026amp; Conditions, and how to grant applications access to AWS services using IAM Roles. Acquired additional knowledge about AWS Database Services: Database Concepts Amazon RDS: A managed relational database service that supports multiple database engines such as MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server. It automates tasks like backup, patching, and scaling. Amazon Aurora: A high-performance, fully managed relational database compatible with MySQL and PostgreSQL, designed for scalability and high availability. Amazon RedShift: A fully managed data warehouse service optimized for large-scale data analysis (OLAP) and big data analytics. Amazon ElastiCache: A managed in-memory caching service supporting Redis and Memcached, improving application performance by reducing database load and query latency. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Complete the labs from module 6. Understand basic AWS services and how to use both the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Lab practice for module 6: + Complete Lab 5 on Amazon Relational Database Service (Amazon RDS) + Complete Lab 43 on schema conversion and database migration 10/13/2025 10/14/2025 https://000005.awsstudygroup.com https://000043.awsstudygroup.com 3 Lab practice for module 6: + Complete Lab 5 on Amazon Relational Database Service (Amazon RDS) + Complete Lab 43 on schema conversion and database migration 10/13/2025 10/14/2025 https://000005.awsstudygroup.com https://000043.awsstudygroup.com 4 - Research and work on the group project 10/15/2025 10/17/2025 5 - Research and work on the group project 10/15/2025 10/17/2025 6 - Research and work on the group project 10/15/2025 10/17/2025 Week 7 Achievements: Successfully completed Lab for module 6\nMaking good progress on researching and working on the group project\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Review lessons to prepare for the midterm exam Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 3 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 4 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 5 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 6 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 Week 8 Achievements: Still in the review process. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Review lessons and take the midterm exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 3 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 4 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 5 - Take the midterm exam 10/30/2025 10/30/2025 6 - Research and work on the group project 10/31/2025 10/31/2025 Week 9 Achievements: Finished reviewing and completed the midterm exam. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learned the basics of EC2, successfully deployed and connected instances, and understood AWS networking services.\nWeek 3: Understood AWS Compute VM services and successfully deployed AWS Backup, File Storage Gateway, S3 bucket, and Autoscaling Group.\nWeek 4: Learned AWS storage services, practiced VM Import/Export and deployed File Storage Gateway, completed the worklog, and finalized the project idea.\nWeek 5: Understood AWS security services and completed labs on EC2 cost optimization and resource management.\nWeek 6: Completed IAM labs, gained basic knowledge of AWS KMS, and explored AWS database services.\nWeek 7: Completed module 6 labs (RDS and database migration) and made progress on researching and implementing the group project.\nWeek 8: In the process of reviewing for the midterm exam.\nWeek 9: Reviewed and completed the midterm exam.\nWeek 10: Continued progress in researching and implementing the group project.\nWeek 11: Continued progress in researching and implementing the group project.\nWeek 12: Continued progress in researching and implementing the group project.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn S3 bucket S3 types such as: + Standard - Standard-IA - S3 One Zone-IA - S3 Glacier Deep Archive 15/09/2025 15/09/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 16/09/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Project Introduction This workshop presents the architecture and implementation of English Journey,\na vocabulary-learning web application built on AWS.\nThe application allows students to:\nsign up and sign in securely, take a level placement test (A1–C1) before learning, practise vocabulary and quizzes, track their own learning progress. On the AWS side, the project demonstrates how to combine several managed services:\nAWS Amplify as the central platform for the web app backend and hosting, Amazon Cognito for authentication, AWS Lambda for backend logic (Level test, Quiz, Vocabulary), Amazon DynamoDB for application data, Amazon SES for sending email notifications and alerts to learners (Account Verification), Amazon CloudWatch for logs, metrics, AWS WAF for basic web application protection, and IAM Roles \u0026amp; Policies to control access between all components. Workshop Objectives By the end of this workshop, a reader should be able to:\nUnderstand the overall architecture of the English Journey web app on AWS. Explain the role of Amplify and how it orchestrates Cognito, Lambda, DynamoDB and S3. Describe how the level-test feature connects frontend, Lambda and DynamoDB. Understand how notifications and system alerts are delivered via email using Amazon SES. Recognise the importance of CloudWatch and IAM for monitoring and security. Workshop Overview This project leverages AWS services to build and deploy the application:\nAWS Amplify: A full-stack hosting service that enables quick and easy deployment of applications. AWS Lambda: Handles application tasks and logic without the need to manage servers, saving costs and resources. Amazon DynamoDB: A NoSQL database used to store user data, vocabulary, and learning results. Amazon S3: Stores learning materials (videos, audio, images) to support the learning process. Amazon CloudWatch: Monitors the performance and operation of the application, providing logs and alerts in case of issues. !\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Migration \u0026amp; Modernization\nCost Optimization in Cloud Computing: Rehost Migration Handbook (Part 4 – Migration: Realizing Cost Savings) Authors: Dan Krueger, James Gaines, and Rohit Vaitheeswaran – 31/03/2025\nCategories: AWS Cloud Financial Management, AWS Compute Optimizer, AWS Cost Explorer, Best Practices, Billing \u0026amp; Account Management, Cloud Cost Optimization, Customer Enablement, Enterprise Strategy, Thought Leadership\nThis article is the fourth part in a four-part step-by-step guide to optimizing costs throughout the Rehost migration process on AWS, specifically covering:\nDiscovering cost components and on-premises environment. Building an accurate business case during the assess phase. Understanding and controlling cloud spend during the mobilize phase. Cost optimization to achieve planned financial savings during the migrate phase. Figure 1: Cost activity overview of the Rehost Migration process by phase\n1. Cost Optimization on AWS: We use specific AWS services and tools throughout the Rehost migration to optimize costs.\nThese AWS services provide recommendations to optimize Amazon EC2 instance costs, storage costs, and overall application run costs. Additional savings may come from initial migration and optimization based on application performance metrics from the on-premises data center (including CPU, memory, disk I/O, and network). Post-migration cost optimization requires analyzing usage metrics, which vary by instance type and size.\n1.1. AWS Billing and Cost Management Home AWS Billing and Cost Management provides essential services for cost optimization throughout the Rehost migration.\nIf using AWS Organizations with a management account for centralized governance, configure member accounts’ access to these services per organizational policies. These services apply to many teams beyond designated cost management stakeholders. The AWS Billing and Cost Management console provides summaries enabling account owners to drill down into specific categories.\nThe landing page includes default widgets offering a quick view of current forecast costs compared to recent months.\nThe Info tab shows detailed summaries explaining each widget for the account owner.\nDefault widgets include:\nCost Summary: Displays current spending trends versus the previous month. Costs shown exclude credits and discounts. Cost Monitor: Shows costs, budgets, and AWS-detected anomalies based on Cost Anomaly Detection configurations. Cost Breakdown: Displays cost analysis over the past six months, grouped by AWS service, member account, region, cost allocation tags, and cost categories. Recommended Actions: Provides guidance for AWS account owners on cloud financial management best practices and cost optimization based on recommendations. Savings Opportunities: Offers suggestions from the Cost Optimization Hub in various categories, such as resizing, instance type changes, and cleanup of unused resources. 1.2. AWS Cost Explorer The Cost Breakdown widget can be expanded to AWS Cost Explorer, allowing account owners and cost managers to visualize, analyze, and manage AWS spending.\nAWS Cost Explorer provides both overview and detailed views of spending trends. Users can filter and forecast to identify cost drivers and anomalies. Reports can be customized by time range (hour, day, month) and grouped by service. Saved reports can be stored for future use.\n1.3. AWS Cost Optimization Hub The AWS Cost Optimization Hub offers an overview of cost optimization opportunities for migrated EC2 instances.\nFigure 2: Cost Optimization Hub and Recommendations\nAfter Rehost migration completes, the system recommends switching to Graviton instances for additional cost savings.\nBased on CPU and memory usage metrics, the hub suggests right-sizing instances and removing inactive resources.\nLeveraging the Cost Optimization Hub helps cost management teams identify, prioritize, and implement effective savings measures, improving cloud financial management and optimizing resources beyond savings achieved during migration.\n1.4. AWS Compute Optimizer AWS Compute Optimizer reduces EC2 instance costs with size-appropriate recommendations. It provides savings overviews, performance improvements, and optimization advice by region for resources including:\nAmazon EC2 Amazon EC2 Auto Scaling Groups Amazon EBS AWS Lambda functions Amazon ECS on AWS Fargate Commercial software licenses Amazon RDS DB instances and storage When enabled, Compute Optimizer evaluates resources based on technical specifications and usage patterns recorded by Amazon CloudWatch over the past 14 days, considering CPU utilization, network traffic, disk activity, and current instance usage levels.\n2. Conclusion Establish dedicated ownership and implement systematic cost monitoring processes using AWS Cost Explorer, AWS Budgets, and AWS Cost and Usage Reports.\nThese tools, combined with AWS Cost Optimization Hub and AWS Compute Optimizer, provide actionable recommendations for EC2 instances, storage, and application optimization. Regular analysis of resource usage patterns and implementing resizing recommendations maintain long-term cost efficiency beyond initial migration savings, laying the foundation for continuous cost optimization.\nThis article concludes the Rehost migration cost optimization journey during the migrate phase by reviewing:\n1.1 AWS Billing and Cost Management Home services, 1.2 AWS Cost Explorer for visualization and cost analysis, 1.3 AWS Cost Optimization Hub and 1.4 AWS Compute Optimizer for detailed cost optimization recommendations and resizing opportunities. Blog Series Direct links to each part of the series:\nPart 1: Assess – Explore Cost Components Part 2: Assess – Build a Business Case Part 3: Mobilize – Understand and Control Cloud Spend Part 4: Migrate – Realizing Planned Savings Additional Resources Contact AWS migration experts to discuss how we can support your organization.\nReady to migrate and optimize costs? Additional resources include:\nAWS Cloud Financial Management Guide to adjust your financial processes for cloud readiness. Latest AWS Cloud Financial Management content. Learn more about migration and modernization with AWS. Explore Amazon Q Developer — an AI-powered assistant helping transform .NET, mainframe, VMware, and Java workloads beyond traditional Rehost models. About the Authors Dan Krueger – Senior Customer Solutions Manager at Amazon Web Services, with extensive experience supporting U.S. Federal Government customers. At AWS, he has led large-scale cloud migration projects helping agencies enhance mission execution capabilities. Previously a Program Executive at IBM focused on data platform modernization and complex technology solutions for government clients.\nJames Gaines – Senior Solutions Architect in Healthcare and Life Sciences at AWS. Experienced in highly regulated environments including the Department of Defense and pharmaceutical industry. Holds all AWS Certifications and specializes in cloud migrations, application modernization, and advanced analytics driving innovation in healthcare and life sciences.\nRohit Vaitheeswaran – Senior Solutions Architect at AWS, specializing in Healthcare and Life Sciences. Experienced in multiple industries leading large-scale migration strategy and cloud optimization initiatives. Has helped organizations in Financial Services, Fintech, and Healthcare optimize their cloud journey on AWS with a focus on migration strategy and cost optimization, maximizing cloud investment and ensuring compliance with industry-specific requirements.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "AWS DevOps \u0026amp; Developer Productivity Blog\nExploring AWS Console: Diagnosing Errors with Amazon Q Developer Authors: Marco Frattallone, Matthias Seeger, and Surabhi Tandon — January 10, 2025*\nCategory: Amazon Q, Amazon Q Developer, AWS Management Console, DevOps, Intermediate (200)\nIntroduction Developers, IT Operators, and sometimes Site Reliability Engineers (SREs) are responsible for deploying and operating infrastructure along with applications, and effectively troubleshooting issues promptly. Effective incident management requires fast diagnosis, root cause analysis, and corrective actions. Identifying root causes can be complex in modern systems, which include many resources deployed distributedly across various environments. Amazon Q Developer, an AI generative-assisted assistant, can simplify this process by diagnosing errors appearing in the AWS Management Console.\nAmazon Q Developer saves crucial time when handling production incidents by supporting diagnosis of errors related to your AWS environment. These errors may stem from misconfigurations across multiple resources, often requiring toggling between different AWS service consoles to find the root cause. Amazon Q Developer uses generative AI to automate the error diagnosis process within the AWS Console interface, reducing Mean Time To Repair (MTTR) and minimizing the impact on business operations.\nThis article explains how Amazon Q Developer supports error diagnosis in the AWS Console when working with supported AWS services. It also describes how this feature works to guide you in troubleshooting. We analyze the behind-the-scenes processes that enable and support this feature.\nDiagnosing with Amazon Q The Diagnose with Amazon Q feature helps diagnose most common errors occurring in the AWS Management Console for AWS services currently supported by this feature. It activates when a user with appropriate permissions clicks the Diagnose with Amazon Q button next to an error message. Amazon Q provides a natural language explanation analyzing the root cause of the error. Then, when the user clicks Help me resolve, Amazon Q displays an ordered list of step-by-step instructions to help you fix the issue. After completion, you can provide feedback on whether the solution provided by Amazon Q was helpful.\nAn example illustrates how Amazon Q Developer can help diagnose an error when launching an Amazon EC2 instance and provide step-by-step guidance to resolve the error.\nDiagnosing with Amazon Q – IAM Permissions related to EC2 instance launch error Behind the Scenes: How Amazon Q Creates Diagnosis To illustrate the concepts, we present explanations in the context of two real examples.\nExample 1: Suppose you try to delete an Amazon S3 bucket that is not empty. This leads to the error message:\nThis bucket is not empty. Buckets must be empty before they can be deleted. To delete all objects in the bucket, use the empty bucket configuration.\nExample 2: Suppose you try to list objects in a specific S3 bucket but lack AWS Identity and Access Management (IAM) permissions to perform the action. This leads to the error message:\nInsufficient permissions to list objects. After you or your AWS administrator has updated your permissions to allow the s3:ListBucket action, refresh the page. Learn more about Identity and access management in Amazon S3.\nWhen you click the Diagnose with Amazon Q button next to the error message in the AWS Management Console, Amazon Q generates an analysis explaining the root cause of the error in natural language. This step is supported by large language models (LLMs). Contextual information provided to the LLM includes the error message displayed in the console, the URL of the triggering action, and the IAM role of the user logged into the AWS Console. The service always operates within the permissions granted to your role when you interact with the AWS Console, and privileges never escalate beyond what you are assigned.\nWhen you click Help me resolve after reviewing the analysis, Amazon Q retrieves additional information about the state of resources in the AWS Account where the error occurred. At this stage, the system proactively determines what information is missing and generates interrogation requests to internal services to fulfill the informational needs. Interrogation is unnecessary for simple errors, such as Example 1 above, but becomes essential for resolving more complex errors when contextual information is insufficient.\nBased on the context, error analysis, user permissions, and account interrogation results, Amazon Q generates step-by-step Resolution instructions. This step is supported by LLMs.\nAfter deploying and validating the steps provided by Amazon Q to resolve the error in the console, you have the option to provide feedback on your experience.\nDiagram illustrating interaction between User, AWS Console, and Amazon Q Developer Contextual Information Contextual information helps large language models (LLMs) generate more accurate and relevant responses. Context data is automatically passed to Amazon Q from the AWS Console interface. As the foundation for the entire analysis and decision-making process, the context information should be provided as fully as possible. At minimum, Amazon Q collects the error message, the URL of the action that caused the error, and the IAM role of the logged-in user.\nThe system automatically extracts relevant identifiers from this context information.\nIn our running Example 1, the URL might be:\nhttps://s3.console.aws.amazon.com/s3/bucket/my-bucket-123456/delete?region=us-west-2\nfrom which Amazon Q extracts aws_region = \u0026quot;us-west-2\u0026quot; and s3_bucket_name = \u0026quot;my-bucket-123456\u0026quot;.\nBeyond this minimal context, Amazon Q may collect additional information from the console related to what the user sees on screen when the error occurs, such as content in text boxes or widgets in the current interface. Amazon Q can also use specific context provided by the underlying service platform. In Example 2, the bucket name is extracted from the URL, the action s3:ListBucket from the error message, and Amazon Q may fetch additional IAM information about relevant policies as well as allow/deny statements.\nInterrogating the Logged-in User’s Account The Diagnose with Amazon Q feature not only passively receives context but can actively query for more information. Amazon Q performs read-only queries to gather additional context about resources in the AWS account, their states, and relationships with the resource experiencing the error. This relationship context helps the LLM model analyze the root cause more accurately when diagnosing errors.\nAmazon Q interrogates the logged-in user’s account using the AWS Cloud Control API (CCAPI) to identify resources deployed in the account. During initial onboarding of Amazon Q, the managed policy AmazonQFullAccess is attached to the IAM role the user employs. This policy includes the IAM permissions cloudformation:ListResources and cloudformation:GetResource for CCAPI, enabling access to the read and list endpoints of CCAPI. If you do not want to attach the managed AmazonQFullAccess policy, you can add cloudformation:ListResources and cloudformation:GetResource directly to your IAM Role.\nIn simple Example 1, where the error occurs because the S3 bucket is not empty, the error message and console URL already contain sufficient information to address it, so no further queries to the AWS account are necessary. In contrast, for the IAM permissions error in Example 2, understanding the IAM role permissions attached to the affected resource is useful. Amazon Q can obtain information about identity-level policies for the role and resource-level policies for the affected resource. Based on that, Amazon Q analyzes the cause of the error by using internal IAM services.\nSpecifically, in Example 2, the URL might be:\nhttps://s3.console.aws.amazon.com/s3/buckets/my-bucket-123456?region=us-west-2\u0026amp;bucketType=general\u0026amp;tab=objects\nfrom which Amazon Q extracts the region and bucket name. It can also extract the s3:ListBucket action directly from the error message. Based on this information, Amazon Q fetches bucket policies for my-bucket-123456 and identity policies for the ReadOnly role, then scans to check whether the s3:ListBucket action is present or calls internal IAM services to collect more information about the cause of the access denial.\nAmazon Q operates only within the permission scope granted by the IAM role of the logged-in user, ensuring privileges never escalate beyond those assigned. Amazon Q calls CCAPI on behalf of the logged-in user, using exactly the permissions allowed by the user\u0026rsquo;s IAM role. CCAPI inherits the user\u0026rsquo;s permissions and has the same level of access to query resources in the user\u0026rsquo;s account.\nIn Example 2, if the logged-in user does not have access to the policies for bucket my-bucket-123456, Amazon Q will not be able to access them either. All API calls are logged in CloudTrail, including Amazon Q’s CCAPI calls, and CCAPI’s calls to target services (e.g., S3, IAM) depending on the request.\nCreating Step-by-Step Resolution Instructions At this stage, all gathered information is synthesized by Amazon Q to create useful and actionable step-by-step remediation instructions. Below are sample instructions for the examples discussed. As models are updated and improved over time, feedback may evolve.\nFor Example 1, sample instructions might be:\nAccess the S3 console, click \u0026ldquo;Buckets\u0026rdquo; and select bucket my-bucket-123456. Click on the \u0026ldquo;Empty\u0026rdquo; tab. If the bucket contains a large number of objects, creating a lifecycle rule to delete all objects in the bucket might be a more efficient way to empty the bucket. Enter \u0026ldquo;permanently delete\u0026rdquo; into the text box and confirm that all objects will be deleted. Try deleting the S3 bucket my-bucket-123456 again. For Example 2, you could:\nGo to the IAM console. Edit the IAM policy attached to the ReadOnly role. Allow the action s3:ListBucket for the resource with ARN: arn:aws:s3:::my-bucket-123456 Save the updated IAM policy. Refresh the S3 console page to list objects in bucket my-bucket-123456. Note that instructions include inferred context such as bucket name my-bucket-123456 rather than placeholders. The instructions returned by Diagnose with Amazon Q are complete and detailed enough for users to follow without additional steps.\nIn practice, although the service uses LLMs to synthesize remediation instructions, Amazon Q applies post-processing to fix common mistakes. For example, in Example 2, the LLM might output the ARN as arn:aws:s3:\u0026lt;region\u0026gt;::\u0026lt;bucket_name\u0026gt;, and the system automatically corrects it as shown above.\nThe returned instructions for Example 2 assume the user cannot list objects due to a missing Allow statement in the policies attached to the ReadOnly role. However, other root causes might be a Deny statement in a policy attached to the S3 bucket or the ReadOnly role. Diagnose with Amazon Q can use the account interrogation process to determine the exact root cause and propose appropriate solutions. For the example above, Amazon Q may fetch the policies attached to the ReadOnly role to check whether the s3:ListBucket permission is missing or retrieve bucket policies attached to bucket-123456.\nValidation One key goal of Diagnose with Amazon Q is to maintain high quality standards, ensuring users always receive helpful and actionable recommendations whenever they encounter errors. An important prerequisite is a strong and flexible evaluation system. Evaluating generative AI systems is challenging due to the large output space (natural language) and non-deterministic responses.\nIn summary, our validation system is built on a large dataset of errors, where each record has a certain number of annotations. Each record contains context (template error messages and Console URLs, e.g., bucket-123456 replaced by {{s3_bucket_name}}, us-west-2 replaced by {{aws_region}}). Annotations include descriptions of the Infrastructure as Code (CloudFormation) representing the account state with errors and the triggering action, as well as expert-provided correct responses. These records allow us to simulate system behavior variants without human interaction and run much faster than real-time by parallelization.\nWe are also developing automated evaluation metrics to compare standard annotations with system responses, enabling fully automated offline evaluations.\nThis validation system allows rapid testing of new ideas by comparing with the current state, preventing quality regressions. Although human experts are still needed to create annotations for error records, we continuously innovate to speed up and simplify this task by developing annotation tools designed to avoid natural language data entry, integrated with validation mechanisms, and requiring output editing rather than full re-annotation.\nConclusion The Diagnose with Amazon Q feature of Amazon Q Developer enables you to identify error root causes in the AWS Console without navigating multiple service consoles. By providing detailed, step-by-step instructions customized to your AWS account and specific error context, Amazon Q Developer helps you troubleshoot and resolve issues effectively. This helps your organization improve operational efficiency, reduce downtime, enhance service quality, and free up valuable human resources to focus on higher-value activities.\nWe also provide insights into how AI and machine learning power this feature behind the scenes.\nAbout the Authors Matthias Seeger\nMatthias Seeger is a Principal Applied Scientist at AWS. His interests include Bayesian learning and probabilistic model-based decision making, theory and applications of Gaussian Process models, probabilistic forecasting, and more recently, large language models (LLMs) and challenges in generating and labeling related data.\nMarco Frattallone\nMarco Frattallone is a Senior Technical Account Manager at AWS, focusing on partner support. He works closely with partners to build, deploy, and optimize solutions on AWS, providing guidance and sharing best practices. Marco is passionate about technology and helping partners lead innovation. Outside of work, he enjoys cycling, sailing, and exploring new cultures.\nSurabhi Tandon\nSurabhi Tandon is a Senior Technical Account Manager at Amazon Web Services (AWS). She supports enterprise customers to achieve high operational efficiency and assists them on their cloud journey with AWS by providing strategic technical guidance. Surabhi is passionate about Generative AI, automation, and DevOps. Outside of work, she enjoys hiking, reading, and spending time with family and friends.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "AWS Database Blog Getting Started with Apache OFBiz and Amazon Aurora DSQL Author: James Morle | Date: March 26, 2025 | Categories: Advanced (300), Amazon Aurora, Best Practices, DSQL, Technical How-to\nWhen designing Amazon Aurora DSQL, we spent a lot of time finding the right balance between innovation and improvements versus ease of use. We did not want to change what was familiar to existing PostgreSQL users unless there was a truly compelling reason. Two areas in this group are optimistic concurrency control (OCC) behavior and the authentication model, where system behavior might feel different to application developers familiar with PostgreSQL.\nIn this article, we present a practical example of migrating an existing application originally running on PostgreSQL to operate with Aurora DSQL. Along with adjustments to adapt to the mentioned aspects, we also handle some data type incompatibilities and work around a few current limitations in Aurora DSQL.\nThe Application To make this example as representative as possible, we chose an open-source application with a broad functional scope and significant complexity, reflecting real-world applications. That application is Apache OFBiz, and the source code can be downloaded from the GitHub repository. The version used in this proof of concept is 18.12.16.\nThe OFBiz application is a full-featured ERP and CRM suite, serving as a typical target for our goals here:\nComplex schema: 837 tables, 4161 indexes Abstract entity engine (Java Persistence, Java Transactions, custom ORM) PostgreSQL support Built-in demo data and data loading functionality Mostly relational model, aligned with Aurora DSQL’s initial compatibility focus The purpose of this article is to combine a description of the internal testing process with some specific code examples on how to approach Aurora DSQL authentication in a connection pool–based Java application. While we provide as much context as possible, this is not intended to be a full script or complete codebase for replicating the entire process.\nAurora DSQL Aurora DSQL is a PostgreSQL-compatible database with some semantic differences aimed at improving scalability and security compared to community PostgreSQL. Some aspects, especially broad functional compatibility with PostgreSQL features, will evolve rapidly over time, but a few core elements define the system. These are the focus of the changes made in the OFBiz application and presented in this article. These include:\nAurora DSQL authentication model — How connections are established and maintained. Transaction size limits — Aurora DSQL does not support unlimited transaction sizes. Isolation level — Aurora DSQL only supports snapshot isolation, equivalent to repeatable read in PostgreSQL. Optimistic concurrency control — Aurora DSQL does not lock resources during transaction construction; it guarantees consistency at commit time. This means some commits may fail and require retries. Data type limitations in keys — Currently, there are some limits on data types (and sizes) supported in primary and foreign keys. These limits will decrease over time as Aurora DSQL adds new supported types. The workarounds presented reflect the limits present in the Aurora DSQL Preview stage. Creating a Data Source Configuration for Aurora DSQL The first step was to define a new data source configuration for Aurora DSQL. We added a new data source named localdsql in the framework/entity/config/entityengine.xml file and set it as the active data source. Parameters were copied from the existing localpostgres data source, with the following key differences underlined:\n\u0026lt;group-map group-name=\u0026#34;org.apache.ofbiz\u0026#34; datasource-name=\u0026#34;localdsql\u0026#34;/\u0026gt; ... \u0026lt;/datasource\u0026gt; \u0026lt;datasource name=\u0026#34;localdsql\u0026#34; helper-class=\u0026#34;org.apache.ofbiz.entity.datasource.GenericHelperDAO\u0026#34; schema-name=\u0026#34;ofbiz\u0026#34; field-type-name=\u0026#34;postgres\u0026#34; check-on-start=\u0026#34;true\u0026#34; add-missing-on-start=\u0026#34;true\u0026#34; use-fk-initially-deferred=\u0026#34;false\u0026#34; alias-view-columns=\u0026#34;false\u0026#34; join-style=\u0026#34;ansi\u0026#34; use-binary-type-for-blob=\u0026#34;true\u0026#34; use-order-by-nulls=\u0026#34;true\u0026#34; result-fetch-size=\u0026#34;50\u0026#34;\u0026gt; \u0026lt;read-data reader-name=\u0026#34;tenant\u0026#34;/\u0026gt; \u0026lt;read-data reader-name=\u0026#34;seed\u0026#34;/\u0026gt; \u0026lt;read-data reader-name=\u0026#34;seed-initial\u0026#34;/\u0026gt; \u0026lt;group-map group-name=\u0026#34;org.apache.ofbiz\u0026#34; datasource-name=\u0026#34;localdsql\u0026#34;/\u0026gt; ... \u0026lt;/datasource\u0026gt; \u0026lt;datasource name=\u0026#34;localdsql\u0026#34; helper-class=\u0026#34;org.apache.ofbiz.entity.datasource.GenericHelperDAO\u0026#34; schema-name=\u0026#34;ofbiz\u0026#34; field-type-name=\u0026#34;postgres\u0026#34; check-on-start=\u0026#34;true\u0026#34; add-missing-on-start=\u0026#34;true\u0026#34; use-fk-initially-deferred=\u0026#34;false\u0026#34; alias-view-columns=\u0026#34;false\u0026#34; join-style=\u0026#34;ansi\u0026#34; use-binary-type-for-blob=\u0026#34;true\u0026#34; use-order-by-nulls=\u0026#34;true\u0026#34; result-fetch-size=\u0026#34;50\u0026#34;\u0026gt; \u0026lt;read-data reader-name=\u0026#34;tenant\u0026#34;/\u0026gt; \u0026lt;read-data reader-name=\u0026#34;seed\u0026#34;/\u0026gt; \u0026lt;read-data reader-name=\u0026#34;seed-initial\u0026#34;/\u0026gt; \u0026lt;/datasource\u0026gt; In the example configuration above, we hid the JDBC URI endpoint — if you perform this on your own cluster, don’t forget to update the value to reflect your Aurora DSQL cluster endpoint.\nWe set the jdbc-password value to the special constant IAM_AUTH. OFBiz by default assumes standard password authentication for database connections, but Aurora DSQL requires AWS Identity and Access Management (IAM)–based authentication for a more integrated and secure environment. We add support for this mechanism in the code in the next step.\nWe also change the isolation level to RepeatableRead (the equivalent PostgreSQL behavior for Aurora DSQL’s snapshot isolation) and increase the minimum pool size (connections in Aurora DSQL are cheap to maintain, but encrypted connection startup is relatively slow). Aurora DSQL only supports encrypted connections, so we add this parameter to the JDBC URI. We also use the latest PG JDBC driver version (42.7.4 at writing) to ensure certified compatibility with Aurora DSQL.\nImplementing Connection Logic for Aurora DSQL There are three key considerations when building connection logic for Aurora DSQL:\nThe actual password is a dynamically generated authentication token, created on demand when IAM authentication is verified. The authentication token has a limited validity period — they are valid only for a specified time window at creation. This period can be up to one week, suitable for development tools, but for production applications, we recommend a much shorter duration to reduce unauthorized access risks. We chose a 3-hour validity here. Connections also have a lifetime, currently one hour. The interplay between token validity and connection lifetime can confuse new Aurora DSQL users, so remember the token is only applied when a connection is created (token must be valid at creation), and connection lifetime affects only existing connections (independent of token validity after creation). To support this, we edited the file framework/entity/src/main/java/org/apache/ofbiz/entity/config/model/EntityConfig.java to add a dedicated authentication flow for Aurora DSQL, adding the following logic:\npublic static String getJdbcPassword(InlineJdbc inlineJdbcElement) throws GenericEntityConfException { String jdbcPassword = inlineJdbcElement.getJdbcPassword(); + if (!jdbcPassword.isEmpty() \u0026amp;\u0026amp; ! jdbcPassword.equals(\u0026#34;IAM_AUTH\u0026#34;)) { return jdbcPassword; } + if (jdbcPassword.equals(\u0026#34;IAM_AUTH\u0026#34;)) { + return DSQLAuth.generateToken(inlineJdbcElement.getJdbcUri().split(\u0026#34;/\u0026#34;)[2], + Region.US_EAST_1, Then, we edited the file framework/entity/src/main/java/org/apache/ofbiz/entity/connection/DBCPConnectionFactory.java to use a dedicated ConnectionFactory for Aurora DSQL when the data source is localdsql. We also set the maximum connection lifetime in the pool slightly shorter than Aurora DSQL’s maximum supported connection lifetime.\n// create the connection factory + org.apache.commons.dbcp2.ConnectionFactory cf = null; + if (cacheKey.equals(\u0026#34;localdsql\u0026#34;)) { + cf = new DSQLConnectionFactory(jdbcDriver, jdbcUri, cfProps); + } else { + cf = new DriverConnectionFactory(jdbcDriver, jdbcUri, cfProps); + } // wrap it with a LocalXAConnectionFactory XAConnectionFactory xacf = new LocalXAConnectionFactory(txMgr, cf); // create the pool object factory PoolableConnectionFactory factory = new PoolableManagedConnectionFactory(xacf, null); + ((PoolableManagedConnectionFactory)factory).setMaxConnLifetimeMillis((long)55*60*1000); factory.setValidationQuery(jdbcElement.getPoolJdbcTestStmt()); Finally, we added core logic to manage connections, including attaching the Aurora DSQL SessionId to connection attributes for better diagnostics. When contacting AWS Support to resolve Aurora DSQL-related issues, providing the Session ID of the specific problematic connection is very helpful. We implemented the following concrete classes:\nframework/entity/src/main/java/org/apache/ofbiz/entity/connection/DSQLAuth.java\nframework/entity/src/main/java/org/apache/ofbiz/entity/connection/DSQLConnectionFactory.java\nframework/entity/src/main/java/org/apache/ofbiz/entity/connection/DSQLConnection.java\nSchema Adjustments We made minimal schema changes to comply with current Aurora DSQL limits. Most of the original schema was compatible, but more adjustments could be made if demo data retention were not required (this will be discussed later). The limitations we worked around relate to supported data types in keys:\nNUMERIC type is currently unsupported in keys:\nWe edited applications/datamodel/entitydef/product-entitymodel.xml, changing the data type of minimumOrderQuantity from fixed-point (NUMERIC(18,6)) to floating-point (float). These two types are not logically identical, but demo data uses decimal. The logic still works correctly here (only “greater than” comparisons used).\nWe edited framework/entity/fieldtype/fieldtypepostgres.xml to change the definition to . Using bigint here is actually more appropriate than NUMERIC(20,0).\nVARCHAR is supported in keys, but the maximum size is smaller than normal VARCHAR:\nWe edited framework/entity/fieldtype/fieldtypepostgres.xml to change the definition to .\nAfter these changes, the schema could be successfully created during data loading (detailed in the next section). We used the -l drop-constraints option in the OFBiz loader utility to ensure no tables were created using foreign key constraints, which are currently unsupported in Aurora DSQL.\nAs mentioned earlier, we could optimize the schema further. A notable change we avoided was converting ID field data types to UUIDs, common in ORM-based applications. Since we are using demo data, we would need to modify all keys (and references) in the demo data to apply UUID types. Instead, we kept the VARCHAR-based keys and loaded the demo data unchanged.\nLoading Demo Data The OFBiz data loader (see OFBiz Technical Setup Guide) originally performed large numbers of inserts before each commit, causing some transactions to exceed Aurora DSQL’s transaction size limits. To fix this, we edited the storeAll() method in framework/entity/src/main/java/org/apache/ofbiz/entity/GenericDelegator.java to batch data loading into groups of 1,000 rows per commit.\nnumberChanged += this.store(toStore); } } + // commit every 1000 rows to conform to DSQL transaction limits + if (numberChanged%1000 == 0) { + if (Debug.verboseOn()) Debug.logVerbose(\u0026#34;Committing at 1000 row threshold. beganTransaction=\u0026#34; + beganTransaction, module); + TransactionUtil.commit(); + beganTransaction = TransactionUtil.begin(); + } } TransactionUtil.commit(beganTransaction); return numberChanged; Note: Aurora DSQL’s actual transaction size limit is larger than 1,000 rows, but we chose this smaller value to observe batching behavior more easily after the change. We also fixed another interesting bug during lookup. Part of the loading logic calls the findKey() function in framework/entity/src/main/java/org/apache/ofbiz/entity/util/EntityCrypto.java to check for the existence of an encryption key in the database. If the key is missing at the expected location, loading stops with an error. When testing with Aurora DSQL, this invariant always triggered. After investigation, we found a general logic bug causing behavior differences between read committed and repeatable read isolation levels. The findKey() method was called without starting a new transaction, so it only saw data as of the current transaction’s snapshot. Since the snapshot was from before the corresponding INSERT committed, findKey() could not see the newly inserted row. Under read committed isolation, the call works fine because the session sees committed data after the transaction start.\nTo fix this, we start a new transaction whenever findKey() is called:\nGenericValue keyValue = null; try { + try { + keyValue = TransactionUtil.doNewTransaction(() -\u0026gt; { + return EntityQuery.use(delegator).from(\u0026#34;EntityKeyStore\u0026#34;).where(\u0026#34;keyName\u0026#34;, hashedKeyName).queryOne(); + }, \u0026#34;checking encrypted key\u0026#34;, 0, true); + } catch (GenericEntityException e) { + throw new EntityCryptoException(e); + } } catch (GenericEntityException e) { throw new EntityCryptoException(e); } We loaded initial seed and demo data following instructions in the OFBiz Technical Setup Guide.\nHandling OCC aborts in transaction command pattern OFBiz submits transactions to the database using a command pattern — with a Callable interface — simplifying Aurora DSQL transaction handling. Other applications (outside OFBiz) that also encapsulate transactions this way can manage retries similarly because transaction boundaries are clear. For illustration, all we add here is a retry loop around the transaction execution, implemented in framework/entity/src/main/java/org/apache/ofbiz/entity/transaction/TransactionUtil.java in the InTransaction.call() method.\nprotected InTransaction(Callable\u0026lt;V\u0026gt; callable, String ifErrorMessage, int timeout, boolean printException) { this.callable = callable; this.ifErrorMessage = ifErrorMessage; this.timeout = timeout; this.printException = printException; } public V call() throws GenericEntityException { + boolean transactionComplete = false; + while (!transactionComplete) { boolean tx = TransactionUtil.begin(timeout); Throwable transactionAbortCause = null; try { try { return callable.call(); } catch (Throwable t) { while (t.getCause() != null) { t = t.getCause(); } throw t; } } catch (Error | RuntimeException e) { transactionAbortCause = e; throw e; } catch (Throwable t) { transactionAbortCause = t; + if (t instanceof SQLException \u0026amp;\u0026amp; (((SQLException) t).getSQLState().startsWith(\u0026#34;OC\u0026#34;) + || ((SQLException) t).getSQLState().equals(\u0026#34;40001\u0026#34;))) { + if (Debug.verboseOn()) { + Debug.logVerbose(\u0026#34;Transaction abort, reason: \u0026#34; + t, module); + Debug.logVerbose(\u0026#34;Retrying on OCC conflict\u0026#34;, module); + } + // Rollback to recover transaction state and loop again + TransactionUtil.rollback(tx, ifErrorMessage, transactionAbortCause); + continue; + } else { throw new GenericEntityException(t); + } } finally { if (transactionAbortCause == null) { TransactionUtil.commit(tx); + transactionComplete = true; } else { if (printException) { Debug.logError(transactionAbortCause, module); } TransactionUtil.rollback(tx, ifErrorMessage, transactionAbortCause); + transactionComplete = true; + } } } + throw new GenericEntityException(\u0026#34;Unexpected state in transaction handler\u0026#34;); } } Note: For production applications, we also recommend adding exponential backoff and jitter to improve stability under high contention. Existing applications that do not model transactions this clearly may require deeper analysis to handle OCC aborts properly. We plan to write a future post on this topic after collecting more customer experience data to identify common patterns.\nConclusion In this article, we guided you through various changes made to a complex existing application. We made minor schema modifications to use supported data types, created new logic to properly interact with Aurora DSQL’s authentication mechanism, implemented batching commits during data loading, and adapted transaction handling to work correctly with OCC semantics.\nWhile this proof of concept focuses more on technical feasibility than production readiness, it demonstrates that Aurora DSQL can support complex database applications like OFBiz with reasonable migration effort. This test confirms that even complex schemas can be adjusted to operate with Aurora DSQL, including improved security and scalability features, through targeted adjustments to authentication, transaction handling, and schema design. These are also key focus areas when planning your own migrations to Aurora DSQL:\nUse supported data types in schemas, including keys\nImplement IAM-based authentication logic required by Aurora DSQL\nEnsure transaction sizes remain within Aurora DSQL limits\nImplement idempotent, retryable transactions\nIf you are working with existing applications on Aurora DSQL, we invite you to share your experiences and observations in the comments!\nAbout the Author James Morle is a Principal Engineer at Amazon Web Services and has been deeply involved in the design and implementation of Aurora DSQL since its inception. He has experience designing and building large-scale databases since the early 1990s.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Tan Luc\nPhone Number: 0379784509\nEmail: lucltse184288@fpt.edu.vn\nUniversity: Ho Chi Minh City FPT University\nMajor: Information Assurance\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/09/2025 08/09/2025 3 - Create AWS Free tier account 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Install \u0026amp; configure AWS CLI + How to use AWS CLI 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn AWS Global Infrastructure and Core Service 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn EC2 and Practice + EC2 Instance + Instance type + \u0026hellip; 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025” Event About the AWS First Cloud Journey Workforce Program Launched in 2021, the program has accompanied more than 2,000 students across the country.\nOver 150 learners have received intensive training and are now working at leading technology companies in Vietnam and internationally.\nMain Objectives: Build a high-quality generation of AWS Builders for Vietnam.\nEquip students with practical skills in Cloud, DevOps, AI/ML, Security, Data \u0026amp; Analytics.\nConnect students with the AWS Study Group community of 47,000+ members and AWS partner enterprises.\nThe program is not only about technology training but also serves as a vital bridge between knowledge – technology – career, helping students confidently integrate into the modern tech world and global community.\nEven Name: Event Name: Kick-off AWS FCJ Workforce – FPTU OJT FALL 2025 Time: 08:30, September 6, 2025 Venue: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City Role in Event:: Participant\nSpeakers and Guests School Representative: Mr. Nguyễn Trần Phước Bảo – Head of Corporate Relations Department (QHDN), Opening Speech Accompanied by 2–3 members from the Corporate Relations Department Keynote \u0026amp; Industry Sharing AWS First Cloud Journey \u0026amp; Future Orientation 👤 Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam DevOps \u0026amp; Future Career 👤 Đỗ Huy Thắng – DevOps Lead, VNG Alumni \u0026amp; Career Sharing From First Cloud Journey to GenAI Engineer 👤 Danh Hoàng Hiếu Nghị – GenAI Engineer, Renova She in Tech \u0026amp; Journey with First Cloud Journey 👤 Bùi Hồ Linh Nhi – AI Engineer, SoftwareOne A Day as a Cloud Engineer 👤 Phạm Nguyễn Hải Anh – Cloud Engineer, G-Asia Pacific Journey to First Cloud Journey 👤 Nguyễn Đồng Thanh Hiệp – Principal Cloud Engineer, G-Asia Pacific ✨ Overal Today’s Kick-off event marks the beginning of the AWS Builders journey – where students not only gain access to the most advanced cloud technologies but also receive inspiration, connect with experts, and expand career opportunities.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "This section summarizes the contents of the workshop you plan to conduct.\nStudying English Website 1. Executive Summary The Studying English Website is designed for learners aiming to improve their vocabulary, grammar, and daily communication skills. The platform leverages AWS Serverless services to provide learning time monitoring, predictive analysis of learners’ abilities to offer learning policies from basic to advanced levels, while minimizing costs.\n2. Problem Statement Current Problem\nEnglish is an essential language for work and daily life. However, learners currently lack space and an environment for practice, especially for communication.\nSolution\nTo address the lack of an English practice environment and support learners in improving vocabulary, grammar, and communication skills, we propose building the Studying English Website on the AWS serverless platform. It enables personalized learning based on user data, integrates listening-speaking exercises and instructional videos with learner recordings stored on S3, tracks and analyzes learning progress via AWS Lambda, ensures security and user management through Cognito, IAM, and rapidly deploys a cost-effective web interface using AWS Amplify, providing a flexible, safe, and effective learning environment while helping administrators improve teaching methods based on real data.\nBenefits and Return on Investment (ROI)\nThe Studying English Website helps learners enhance their English skills in a personalized and flexible way, reducing time and cost compared to traditional learning methods. It also provides learning progress analytics to administrators for optimized teaching methods. With low AWS infrastructure costs (~6.45 USD/month), the project has a fast ROI potential through increased learning efficiency and expanded user base, while creating valuable data for AI projects and long-term analytics.\n3. Solution Architecture The architecture of the Studying English Website is based on the AWS serverless platform, using S3 to store raw and processed data, Amplify Gen 2 for web interface deployment Route53 for DNS and routing management, Cognito for user authentication and management, Secrets Manager for sensitive information security, IAM for access control, Lambda for event-driven serverless logic, and WAF to protect the application from attacks, creating a flexible, personalized, secure, and scalable English learning system.\nAWS Services Used\nAWS Amplify Gen 2: Host the web interface AWS Route53: Manage DNS and routing AWS Cognito: Authenticate and manage users AWS IAM: Manage AWS access permissions AWS Lambda: Run serverless code triggered by events AWS WAF: Protect the web application from attacks AWS SES: Send user verification via Email Component Design\nData Ingestion: Data from users and sources is sent to AWS Lambda, which triggers processing workflows. Data Storage: Raw and processed data are stored in many separate S3 buckets, forming a data lake and ready-to-analyze data repository. Data Processing: AWS Lambda handles serverless events, MediaConvert converts video/audio, and data is indexed. Web Interface: AWS Amplify Gen 2 hosts a Next.js application providing dashboards, real-time analytics, and user data access. User Management: Amazon Cognito handles authentication and user access management, combined with AWS IAM for service access control, securing sensitive information via AWS Secrets Manager, and protecting the entire application with AWS WAF. DNS and routing are managed by Route53. 4. Technical Deployment Deployment Phases\nThe project consists of two parts — building the Studying English Website — each with four phases:\nResearch and Architecture Design: Study and design AWS Serverless architecture (1 month before internship). Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate infrastructure costs and adjust services to ensure feasibility and cost-efficiency (Month 1). Architecture Adjustment for Cost/Performance Optimization: Refine services (e.g., optimize Lambda, MediaConvert, Amplify) and data workflows for maximum efficiency (Month 2). Development, Testing, Deployment: Deploy AWS services using CDK/SDK, develop Next.js interface on Amplify, test the system, and put it into operation (Month 2–3). Technical Requirements\nThe system requires a stable internet connection to operate AWS services, including storing and retrieving data on S3, serverless data processing via Lambda, deploying Next.js web interface on Amplify Gen 2, DNS and routing management via Route53, user authentication and access control with Cognito, sensitive information security via Secrets Manager, service access control via IAM, WAF application protection, as well as supporting data analytics and real-time dashboards.\n5. Roadmap \u0026amp; Milestones Before Internship (Month 0): Study plan preparation Internship (Month 1–3): Month 1: Learn AWS and upgrade hardware Month 2: Learn deployment, plan and design architecture Month 3: Deploy, test, and launch Post Deployment: Research potential development and new features 6. Budget Estimation See costs on AWS Pricing Calculator\nOr download budget estimation file.\nInfrastructure Costs\nAWS Amplify Gen 2: 0.35 USD/month (256 MB, 500 ms request) AWS Route53: 0.50 USD/month (1 domain, 1 million queries) AWS Cognito: 0.00 USD/month (5 Free Tier users) AWS IAM: 0 USD/month AWS Lambda: 0.00 USD/month (1,000 requests, 512 MB RAM) AWS WAF: 5.00 USD/month (1 basic Web ACL) Total: 5.85 USD/month, ~70.2 USD/12 months\n7. Risk Assessment Risk Matrix\nServer downtime: High impact, medium probability Budget overrun: Medium impact, high probability Mitigation Strategy\nCosts: Use AWS Budget for alerts, optimize services Contingency Plan\nRevert to manual data collection if AWS services fail. 8. Expected Outcomes Technical Improvement: Real-time data and analytics replace manual processes. Scalable to 10–15 stations.\nLong-term Value: One-year data platform for AI research, reusable for future projects.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research and work on the group project 11/03/2025 11/07/2025 3 - Research and work on the group project 11/03/2025 11/07/2025 4 - Research and work on the group project 11/03/2025 11/07/2025 5 - Research and work on the group project 11/03/2025 11/07/2025 6 - Research and work on the group project 11/03/2025 11/07/2025 Week 10 Achievements: Making good progress in researching and working on the group project. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research and work on the group project 11/10/2025 11/14/2025 3 - Research and work on the group project 11/10/2025 11/14/2025 4 - Research and work on the group project 11/10/2025 11/14/2025 5 - Research and work on the group project 11/10/2025 11/14/2025 6 - Research and work on the group project 11/10/2025 11/14/2025 Week 11 Achievements: Making good progress in researching and working on the group project. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attend AWS Cloud Mastery Series #2 event 11/17/2025 11/17/2025 3 - Continue researching and working on the group project 11/18/2025 11/21/2025 4 - Continue researching and working on the group project 11/18/2025 11/21/2025 5 - Continue researching and working on the group project 11/18/2025 11/21/2025 6 - Continue researching and working on the group project 11/18/2025 11/21/2025 Week 12 Achievements: *Making good progress in researching and working on the group project.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;CognitoPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-idp:SignUp\u0026#34;, \u0026#34;cognito-idp:InitiateAuth\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:ListFunctions\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:UpdateFunctionCode\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:region:account-id:table/your-table-name\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SESPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudwatch:PutMetricData\u0026#34;, \u0026#34;cloudwatch:GetMetricData\u0026#34;, \u0026#34;cloudwatch:DescribeAlarms\u0026#34;, \u0026#34;cloudwatch:SetAlarmState\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;WAFPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;wafv2:CreateWebACL\u0026#34;, \u0026#34;wafv2:UpdateWebACL\u0026#34;, \u0026#34;wafv2:GetWebACL\u0026#34;, \u0026#34;wafv2:ListWebACLs\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:ListPolicies\u0026#34;, \u0026#34;iam:GetRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Technical background This workshop assumes that the reader has basic knowledge of:\nWeb development\nHTML, CSS and JavaScript/TypeScript React or another single-page application framework Cloud and AWS fundamentals\nwhat an AWS region and account are, the idea of managed services (Cognito, Lambda, DynamoDB,…), basic concepts of IAM (identity, role, policy, least privilege). The report is written so that it can be understood even if the reader does not actually deploy the system, but this background helps to follow the architecture.\nTools and services To reproduce the workshop in a real environment, the following tools and services would be required:\nAn AWS account with permission to create:\nAmplify apps, Cognito User Pools, Lambda functions, DynamoDB tables, SES identities and configuration sets, IAM roles and policies. Node.js and npm installed locally\n(for running and building the React frontend).\nThe AWS CLI configured with an IAM user or role that has sufficient permissions.\nOptionally, the Amplify CLI / Gen 2 tooling\nto define infrastructure in code and connect the project to Amplify.\nSource code and project structure The English Journey project is organised as:\na React frontend (pages such as Level Test, Dictionary, Vocabulary, My Learning), a backend defined via Amplify (Cognito, Lambda, DynamoDB), additional infrastructure for SES (email), CloudWatch and WAF. In this Hugo workshop site, we only present the architecture diagrams, explanations and example code. The actual AWS resources do not need to be created to understand the design decisions.\nThe following sections (5.3 and onwards) build on these prerequisites and explain each group of AWS services in more detail.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-create-amplify/",
	"title": "Create Amplify backend",
	"tags": [],
	"description": "",
	"content": "Goal In this step we describe how the Amplify backend for the English Journey web application was created.\nInstead of provisioning Amazon Cognito, AWS Lambda, Amazon S3, AWS WAF and Amazon DynamoDB separately in the console, we use AWS Amplify (Gen 2) as a central orchestration layer. Amplify reads the configuration from our project and generates the necessary AWS resources for authentication, APIs, data storage and hosting.\n5.3.1 Why Amplify? English Journey is a single-page application built with React. We chose Amplify because:\nit provides one entry point to configure the backend for a web or mobile app, it can automatically create Cognito, Lambda, DynamoDB, S3 and CloudFront resources from a simple configuration, it integrates easily with the Amplify JavaScript libraries used by the frontend (sign-in, API calls, file storage). Amplify becomes the “backend platform” that hides much of the low-level boilerplate of setting up these services one by one.\n5.3.2 Amplify app creation and hosting We created a new Amplify App from the AWS Management Console and connected it to our Git repository that contains the React source code of English Journey. During the wizard we configured: the default build settings (install dependencies and run npm run build), the environment variables required by the app. Amplify automatically created: an S3 bucket to store the built static files, a CloudFront distribution in front of that bucket to serve the web site with low latency. The output of this step is a public URL where the React frontend of English Journey is hosted. All later backend resources (Cognito, Lambda, DynamoDB…) are associated with this Amplify app. 5.3.3 Authentication with Cognito (Amplify Auth) To implement sign-up, sign-in and password reset for students, we enabled the Auth category in Amplify.\nConceptually, the steps were:\nDefine the authentication requirement in Amplify: sign-in with email and password, self-registration enabled so that new students can create accounts, basic password policy and email verification. Amplify generated an Amazon Cognito User Pool with the corresponding settings. The React frontend uses the Amplify Auth library to: create new users (sign up), authenticate existing users (sign in), read user attributes (name, email) and display greetings such as “Chào mừng trở lại, Duy Khang!”. All tokens issued by Cognito (ID token and access token) are later used to protect our API and Lambda functions.\n5.3.4 Backend logic with Lambda (Amplify Functions) The main business logic of English Journey runs in AWS Lambda.\nUsing Amplify’s Function category we created several Lambda functions, for example:\nMyLearning / DailyCheckIn – updates study streaks and progress for the user. LevelTest – receives the answers from the placement test, calculates the CEFR level (A1–C2) and stores the result. Dictionary / Vocabulary – provides APIs for searching words, saving “bookmarked” vocabulary and tracking which words a user has mastered. From the Amplify project these functions are defined as backend handlers.\nWhen deploying the Amplify app, each function is created as a separate Lambda with the correct IAM permissions and environment variables (for example, DynamoDB table names).\n5.3.5 Data layer with DynamoDB To store application data we defined several data models in the backend, which Amplify maps to Amazon DynamoDB tables:\nUsers / Profiles – basic user information and learning preferences. PlacementTestResults – scores and detected level for each attempt. Vocabulary / Dictionary – list of words, meanings, examples and CEFR levels. UserProgress – saved vocabulary, words marked as “mastered”, quiz history, daily streaks. Each Lambda function receives the table name via environment variables generated by Amplify, and accesses DynamoDB through the AWS SDK.\nUsing Amplify in this way keeps all table definitions in code and makes the deployment reproducible.\n5.3.7 Protection with AWS WAF The public endpoint of the web application is the CloudFront distribution created by Amplify.\nTo protect this endpoint we associated an AWS WAF Web ACL with the distribution and enabled:\nthe AWS managed rule groups that block common web attacks (SQL injection, XSS, bot traffic), a basic rate-limit rule to prevent simple denial-of-service attempts. In the architecture diagram this is represented by the AWS WAF component in front of the Amplify application.\nSummary In summary, Amplify is the central service that creates and connects:\nCognito for authentication, Lambda for backend logic, DynamoDB for application data, and integrates with AWS WAF for additional protection. The rest of the workshop (SNS, CloudWatch, IAM policies, …) builds on top of this Amplify-managed backend.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Cost Optimization in Cloud Computing: Rehost Migration Handbook (Part 4 – Migration: Realizing Cost Savings) This blog is part 4 of a four-part series on cost optimization during Rehost Migration to AWS, focusing on the migrate phase - realizing the planned cost savings. The article introduces 4 main AWS tools for post-migration cost optimization: AWS Billing and Cost Management (monitoring and analyzing overall costs), AWS Cost Explorer (visualizing spending trends), AWS Cost Optimization Hub (providing optimization recommendations such as switching to Graviton instances, rightsizing, removing unused resources), and AWS Compute Optimizer (suggesting rightsizing for EC2, EBS, Lambda, RDS based on actual metrics). The goal is to help businesses not only achieve initial savings but also maintain continuous cost optimization through regular analysis and implementation of appropriate rightsizing recommendations.\nBlog 2 - Exploring AWS Console: Diagnosing Errors with Amazon Q Developer This blog introduces the \u0026ldquo;Diagnose with Amazon Q\u0026rdquo; feature of Amazon Q Developer - an AI assistant that helps diagnose and troubleshoot errors directly in the AWS Management Console. When encountering an error, users simply click the \u0026ldquo;Diagnose with Amazon Q\u0026rdquo; button, and the system uses LLM (Large Language Models) to analyze the root cause based on contextual information (error message, URL, IAM role). The \u0026ldquo;Help me resolve\u0026rdquo; feature then provides detailed step-by-step remediation guidance. Amazon Q operates by: collecting contextual information from the console, proactively querying additional data via AWS Cloud Control API (within the user\u0026rsquo;s IAM permissions), and synthesizing it into specific solutions. The goal is to reduce MTTR (Mean Time To Repair), save troubleshooting time, and help developers/operators handle incidents more efficiently without navigating across multiple consoles.\nBlog 3 - Getting Started with Apache OFBiz and Amazon Aurora DSQL This blog presents a real-world case study of migrating Apache OFBiz (open-source ERP/CRM with 837 tables, 4161 indexes) from PostgreSQL to Amazon Aurora DSQL. Key changes include: (1) Implementing IAM-based authentication instead of password authentication, with time-limited tokens and connection pooling management; (2) Schema adjustments - changing NUMERIC data types to BIGINT, reducing VARCHAR size in keys due to current Aurora DSQL limitations; (3) Handling transaction limits - splitting batch data loading into batches of 1000 rows per commit; (4) Handling OCC (Optimistic Concurrency Control) - adding retry logic for transactions that may abort due to conflicts, as Aurora DSQL only supports snapshot isolation and doesn\u0026rsquo;t lock resources during transaction building. The results demonstrate that Aurora DSQL can support complex applications with reasonable migration effort, focusing on 4 key aspects: using supported data types, IAM authentication, transaction size limits, and idempotent retry-able transactions.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-aws-cognito/",
	"title": "Configure AWS Cognito",
	"tags": [],
	"description": "",
	"content": "Goal In this step, we describe how Amazon Cognito is used in the English Journey web application to manage user authentication.\nStudents can sign in using:\nEmail \u0026amp; password Google (Gmail) account Cognito acts as the central identity provider that issues tokens for the frontend and backend of English Journey.\n5.4.1 Role of Amazon Cognito in the architecture In the architecture of English Journey:\nCognito User Pool stores user identities (email, name, etc.). It handles sign-up, sign-in, password reset and email verification. It integrates with Amplify so the React frontend can easily call signIn, signUp and other auth functions. It also connects to social identity providers: Google → for users who want to log in with their Gmail All successful logins (email, Google) result in Cognito tokens that are used to protect our APIs and Lambda functions.\n5.4.2 Creating the Cognito User Pool The main configuration steps:\nCreate a User Pool\nSign in to the AWS Management Console → Amazon Cognito → User pools → Create user pool. Choose Email as the primary sign-in identifier. Allow self-registration so students can create their own accounts. Configure password policy \u0026amp; verification\nSet a basic password policy (minimum length, characters, etc.). Enable email verification so students must confirm their email before using the app. Customize email templates (optional) for sign-up and password reset. Create an app client\nCreate a public app client for the web frontend. Enable Cognito User Pool and social IdPs as allowed identity providers. Configure allowed callback URLs and sign-out URLs (for example, the Amplify frontend domain of English Journey). This user pool is later referenced by Amplify and the React frontend.\n5.4.3 Enabling Google (Gmail) To support social logins, we added Google as identity providers in Cognito.\nGoogle (Gmail) login In Google Cloud Console, create an OAuth 2.0 Client ID for a web application. Set the authorized redirect URI to the Cognito callback URL generated for the user pool. Copy the Client ID and Client Secret from Google. In Cognito → User pool → Identity providers → Google: Paste the Client ID and Client Secret. Map Google attributes (email, name) to Cognito standard attributes. Now users can click \u0026ldquo;Sign in with Google\u0026rdquo; and authenticate using their Gmail account.\n5.4.4 Integrating Cognito with the Amplify frontend The React frontend of English Journey uses AWS Amplify Auth to communicate with Cognito.\nConceptually:\nFor email/password: Auth.signUp() is used to create a new user. Auth.signIn() is used for normal login. For Gmail social login: We call Auth.federatedSignIn({ provider: 'Google' }), Amplify redirects the user to Google, then back to Cognito, then back to the web app with valid tokens. On the UI, the login page shows three main options:\nSign in with email \u0026amp; password\rSign in with Google\rAll three methods still end up in the same Cognito User Pool.\n5.4.5 Security and user management With Cognito, we can:\nRequire email verification before granting full access to the application.\nRestrict which domains are allowed to use the sign-in flow (via callback URLs).\nManage users centrally:\nLock accounts Reset passwords Delete accounts Extend the solution later with:\nMFA (Multi-Factor Authentication) Additional social providers (GitHub, Facebook, …) Within the scope of this workshop, we focus on:\nSign-in with email \u0026amp; password Application login via Gmail Email verification (OTP) Summary In this step, we configure Amazon Cognito as the identity management service for English Journey:\nCreate a User Pool to manage accounts and authenticate users. Allow users to sign in with email and Google (Gmail). Use tokens issued by Cognito in the React + Amplify front end to securely access APIs and back-end services. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nTime: 08:30 on 06/09/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\nEvent 2 Event name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nTime: 14:00 on 03/10/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\nEvent 3 Event name: AI/ML/GenAI on AWS\nTime: 8:30 on 15/11/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\nEvent 4 Event name: DevOps on AWS\nTime: 8:30 on 17/11/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\nEvent 5 Event name: According to AWS Well-Architected Security Pillar\nTime: 08:30 on 29/11/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendee\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.5-create-ses/",
	"title": "Configure Amazon SES for email",
	"tags": [],
	"description": "",
	"content": "Goal SES will be used to send transactional emails to learners\nIt can later be extended to:\nSend welcome emails when an account is created. Send study reminder emails. Note: SES is a regional service. Make sure you are working in the same AWS Region that you used for Amplify, Lambda and DynamoDB.\n5.5.1 – Open the Amazon SES console From the AWS Management Console, type “SES” in the search box. Select Amazon Simple Email Service. Check the Region in the top-right corner (for example: ap-southeast-1). If SES is in a different Region, switch to the Region used for this workshop environment. 5.5.2 – Verify an email identity For this workshop we will verify a single email address and send emails from that address (for example: your personal Gmail).\nIn the SES left navigation menu, choose Verified identities. Click Create identity. Select Email address. In Email address, enter the address you will use as the sender, for example:\nyour-name+english-journey@gmail.com. Leave the other options as default and choose Create identity. SES sends a verification email to that address. Open your inbox, find the email from Amazon Web Services, and click the verification link. Return to the SES console and refresh. The identity status should become Verified. From now on, SES will only allow sending emails from and to verified identities (SES Sandbox mode). This is enough for a classroom / workshop environment.\n5.5.3 – (Optional) Move out of Sandbox mode If later you want to send emails to real learners (unverified addresses), you must move your SES account out of sandbox mode:\nIn the SES console, go to Account dashboard. Under Your account details, check the Account status. If it is still Sandbox, click Request production access and follow the instructions. Within the scope of this workshop, you can stay in sandbox mode as long as you only send emails between verified addresses.\n5.5.4 – Create a configuration set (optional but recommended) A configuration set groups all emails from the English Journey application and enables future observability (CloudWatch metrics, event publishing, etc.).\nIn the SES navigation menu, choose Configuration sets. Click Create configuration set. Enter a name, for example: english-journey-config. Leave all other settings as default and click Create configuration set. We will use this configuration set later when sending emails from Lambda functions.\n5.5.5 – Allow Lambda to send email with SES Lambda functions such as Daily Check-in or Test Level result will send emails through SES.\nLambda needs permission to call the SES APIs.\nYou will attach this permission in Section 5.7 – Create IAM Roles \u0026amp; Policies, but we prepare the policy here for reference:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "English Journey Overview English Journey is an innovative web application designed to help users learn English vocabulary in a structured and interactive way. This platform leverages various AWS services to provide a smooth learning experience, allowing users to track their progress, engage with dynamic content, and receive personalized feedback.\nKey Features: User Authentication: Using Amazon Cognito, users can register, log in, and securely access their learning materials.\nInteractive Learning Modules: The app offers various interactive lessons to help users expand their vocabulary.\nProgress Tracking: Users can track their progress and completion of vocabulary lessons, with detailed reports generated through AWS Lambda and DynamoDB.\nNotifications: Email notifications via Amazon SES will inform users about new lessons, progress milestones, account updates, and other important changes.\nContent Storage: All learning materials are securely stored in Amazon S3 with proper access control.\nWeb Security: To protect the platform, AWS WAF ensures the application is protected from common web threats.\nMonitoring and Alerts: AWS CloudWatch is used to monitor platform performance, with alerts configured for potential issues.\nTechnologies Used: Frontend: Built using modern web technologies, ensuring a smooth and responsive user experience.\nBackend: Powered by AWS services like Lambda and DynamoDB, ensuring scalability and performance.\nStorage: All data and media content are securely stored in Amazon S3.\nContent Workshop overview Prerequiste Create Amplify AWS Cognito Configure Amazon SES for email CloudWatch IAM Roles - Policies Configure Amazon Route 53 Clean up "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.6-create-cloudwatch/",
	"title": "Configure Amazon CloudWatch",
	"tags": [],
	"description": "",
	"content": "Goal To understand how the application behaves in production and to be able to react quickly to errors, we use Amazon CloudWatch for:\ncollecting logs from Lambda functions, monitoring metrics (invocations, errors, throttles, latency), 5.6.1 CloudWatch Logs for Lambda and API By default, every Lambda function created by Amplify writes logs to CloudWatch Logs.\nIn this project, logs are used to:\nTrack requests for: Level test submissions, Quiz submissions, Dictionary / vocabulary lookups, Debug input errors, permission (IAM) errors or timeouts, Record important application events. In addition to the default logs, some functions write structured JSON logs, which makes it easier to search by userId, requestId or feature.\n5.6.2 Metrics for key services CloudWatch automatically provides metrics for:\nLambda – invocations, errors, duration, concurrent executions, DynamoDB – read/write capacity, throttled requests, S3 / CloudFront – data transfer and request counts, WAF – number of allowed/blocked requests. For the workshop we focus on a few key metrics:\nLambda Error count and Error rate for the main backend functions, Lambda Duration to detect performance issues in level test \u0026amp; quiz processing, DynamoDB ThrottledRequests to see if the provisioned capacity is sufficient. 5.6.3 CloudWatch Dashboard (optional) To quickly observe the system health, the team creates a small CloudWatch Dashboard showing:\nA chart of Lambda error rate over time, Execution duration of the LevelTest and Dictionary functions, (Optional) Number of requests blocked by AWS WAF. The dashboard is not mandatory for the workshop, but it helps illustrate the system behavior when many students are using the application.\nSummary CloudWatch completes the monitoring story for English Journey by providing:\nLogs for analysis and debugging, Metrics \u0026amp; dashboards to observe trends, Combined with Amplify, SES and WAF, this gives a reasonably robust operational setup for this workshop project.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon from 09/08/2025 to 12/08/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in internship and learning about cloud, through which I improved my skills in programming, cloud applications, analysis, report writing, communication, etc...\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Agility and faster learning ability Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.7-create-iam-roles-policies/",
	"title": "Create IAM Roles-Policies",
	"tags": [],
	"description": "",
	"content": "Goal This section explains how IAM roles and policies are designed for the English Journey application.\nMost roles are generated by AWS Amplify, but we still need to understand:\nwhich roles exist, what each role is allowed to do (S3, DynamoDB, SES, MediaConvert, …), and how we apply the least-privilege principle. 5.7.1 Overview of IAM in the architecture In the architecture from sections 5.3–5.6, IAM is the glue that connects services:\nAmplify uses IAM roles to deploy CloudFormation stacks and host the frontend. Lambda functions use execution roles to access DynamoDB, S3 and SES (for sending emails). CloudWatch and SES rely on IAM so alarms and email notifications can be sent correctly. The design goal is that each component only receives the minimum permissions it needs.\n5.7.2 Lambda execution roles When we define backend functions in Amplify (Level Test, My Learning, Dictionary, Vocabulary, …), Amplify automatically creates a Lambda execution role for each function.\nEach role has:\nTrust policy – allows the Lambda service to assume the role:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? I was most satisfied with the supportive environment and clear guidance from my mentor, which allowed me to learn and grow while working on tasks related to my field of study. What do you think the company should improve for future interns? More networking or team bonding events outside of working hours, and regular feedback sessions to track progress and improvements. If recommending to a friend, would you suggest they intern here? Why or why not? Yes, I would recommend it. The supportive work environment, learning opportunities, and flexible working hours are great advantages. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? More team-building events and regular feedback sessions would enhance the internship experience. Would you like to continue this program in the future? Yes, I would love to continue. The experience has been valuable, and I would like to gain more exposure. Any other comments (free sharing): The internship has been great overall. Expanding the scope of tasks for interns would make it even better. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.8-route-53/",
	"title": "Configure Amazon Route 53",
	"tags": [],
	"description": "",
	"content": "5.8 Configure Amazon Route 53 (Custom Domain) In this step we connect the Amplify-hosted English Journey frontend to a custom domain managed by Amazon Route 53.\n🔗 Domain used in this workshop\nFor the demo environment we use the domain\nenglishjourney.xyz – the final site is available at:\nhttps://www.englishjourney.xyz/\n5.8.1 Create / verify the hosted zone Open the Route 53 console → Hosted zones → Create hosted zone. Enter your domain name, e.g. englishjourney.xyz, and keep type = Public hosted zone. Route 53 creates a set of NS and SOA records for the zone. If the domain is registered elsewhere, copy the Route 53 NS records to your registrar so that DNS is delegated to Route 53. 5.8.2 Connect the domain in AWS Amplify Go to the AWS Amplify console → select your English Journey app.\nIn the left menu choose Domain management → Add domain.\nSelect the hosted zone englishjourney.xyz.\nMap the root and sub-paths, for example:\nenglishjourney.xyz → main branch (production) www.englishjourney.xyz → redirect to root Amplify automatically creates the required A / AAAA and CNAME records in Route 53.\n5.8.3 Test the site Wait for DNS and SSL provisioning to complete (a few minutes).\nOpen a browser and navigate to:\nhttps://www.englishjourney.xyz/ Verify that the English Journey homepage is served correctly over HTTPS.\nNote this URL in your report / slides as the public entry point of the workshop application.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.9-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Goal This section explains how to remove the AWS resources that were created or used during the English Journey workshop, so that you do not incur unexpected costs.\nYou should only delete these resources when you have finished experimenting with the architecture.\n5.9.1 – Delete the Amplify app and front-end hosting Open the Amplify console in the same Region used for the workshop. Select the Amplify app that is hosting the English Journey front end. Choose Actions → Delete app (or the Delete button in the app details page). Confirm the deletion as instructed. When you delete the Amplify app:\nAmplify automatically removes the front-end hosting, And usually deletes the backend stacks it created (Cognito, Lambda, DynamoDB),\nunless you explicitly choose to keep them during the deletion process. Carefully read the confirmation dialog to avoid deleting important resources by mistake.\n5.9.2 – Delete any remaining back-end resources Depending on how you created the back end, there may still be some resources left after deleting the Amplify app.\nIn the AWS console, in the workshop Region, check the following services:\nCognito\nDelete any User Pools or Identity Pools that were created specifically for the workshop. Lambda\nDelete Lambda functions that only serve English Journey (for example: level test handlers, daily reminders, vocabulary processing). DynamoDB\nDelete DynamoDB tables that were used only for workshop data (learning progress, questions, vocabulary, …) if you no longer need them. 5.9.3 – Clean up SES, CloudWatch and WAF In addition to the core backend, this workshop uses Amazon SES, CloudWatch and optionally AWS WAF.\nAmazon SES Open the Amazon SES console. In Verified identities: Delete email identities that were created only for the workshop (for example: test sender or test recipient addresses). In Configuration sets: Delete the configuration set used by the English Journey application (for example: english-journey-config), if you will not reuse it. If your account was moved out of SES sandbox only for the workshop, you may want to review your SES sending quotas and usage, but there is nothing extra to delete for that.\nCloudWatch Open the CloudWatch console. In Log groups, delete: Log groups for Lambda functions that belong to English Journey. AWS WAF If you deployed a dedicated WAF Web ACL for the English Journey frontend:\nOpen the AWS WAF console. Identify the Web ACL associated with the workshop CloudFront distribution or Amplify app. If the Web ACL is used exclusively for this workshop, delete it. 5.9.4 – Clean up IAM roles and policies Finally, review IAM to ensure there are no unused roles or policies left behind:\nIn the IAM console, go to Roles:\nLook for roles created only for this workshop (for example: custom Lambda execution roles, or roles with names that clearly reference English Journey or the workshop). Before deleting a role, confirm that no Lambda function, service or user still depends on it. In Policies:\nRemove customer-managed policies that were created solely for the workshop, especially: policies that grant ses:SendEmail / ses:SendRawEmail to Lambda, policies used only by temporary roles. Do not delete shared or production IAM roles / policies that might be reused by other applications.\nAfter these steps, the AWS environment should no longer contain resources that were created specifically for the English Journey workshop.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]